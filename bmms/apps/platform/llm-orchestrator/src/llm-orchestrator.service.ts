import { Injectable } from '@nestjs/common';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { mkdir, writeFile } from 'fs/promises';
import * as path from 'path';
import { z } from 'zod';
import { LlmChatResponse } from './llm-orchestrator/llm-orchestrator.interface';
import { CodeSearchService } from './service/code-search.service';
import { LlmOutputValidator } from './validators/llm-output.validator';
import { K8sIntegrationService } from './service/k8s-integration.service';

const LLMReplySchema = z.object({
  proposal_text: z.string(),
  changeset: z.object({
    model: z.string(),
    features: z.array(
      z.object({
        key: z.string(),
        value: z.union([z.string(), z.number(), z.boolean()]),
      }),
    ),
    impacted_services: z.array(z.string()),
  }),
  metadata: z.object({
    intent: z.string(),
    confidence: z.number(),
    risk: z.enum(['low', 'medium', 'high']),
  }),
});

type LLMReply = z.infer<typeof LLMReplySchema>;

const SYSTEM_PROMPT = `You are an expert business analyst that converts Vietnamese business model requests into JSON ChangeSet for Kubernetes deployment automation.

**BUSINESS MODELS:**
1. **Retail Model**: One-time purchase, inventory management
   - Required services: OrderService, InventoryService
   - BillingService mode: ONETIME
   - Note: 1 OrderService handles ALL retail products via database (product_id)
   
2. **Subscription Model**: Recurring payment, subscription plans
   - Required services: SubscriptionService, PromotionService
   - BillingService mode: RECURRING
   - Note: 1 SubscriptionService handles ALL subscription plans via database
   
3. **Freemium Model**: Free tier with optional paid add-ons
   - Required services: SubscriptionService (with is_free=true), PromotionService
   - BillingService mode: FREEMIUM (free base + pay for add-ons)
   - Add-ons: Extra storage, premium features, etc. (charged separately)
   - Note: Same SubscriptionService handles free users + add-on purchases
   
4. **Freemium + Add-on Model**: Free base plan with purchasable add-ons
   - Base plan: Free (no billing)
   - Add-ons: Paid features billed separately (e.g., extra storage, AI features)
   - BillingService mode: ADDON (only bill for add-ons, not base subscription)
   
5. **Multi-Model**: Support multiple models simultaneously
   - Required services: ALL of the above
   - BillingService mode: HYBRID (handle all billing types)
   - Note: SHARED SERVICE PATTERN - Each service type deploys ONCE, not per product
   - Example: 2 retail products + 1 subscription ‚Üí Still only 1 OrderService, 1 SubscriptionService

**CORE SERVICES (always needed):**
- AuthService, CustomerService, CRMOrchestratorService
- APIGatewayService
- CatalogueService (Product domain)
- BillingService, PaymentService (Finance domain)

**SERVICE MAPPING:**
- OrderService ‚Üí order-svc (namespace: order, port: 3011)
- InventoryService ‚Üí inventory-svc (namespace: order, port: 3013)
- SubscriptionService ‚Üí subscription-svc (namespace: order, port: 3012)
- PromotionService ‚Üí promotion-svc (namespace: product, port: 3009)
- CatalogueService ‚Üí catalogue-svc (namespace: product, port: 3007)
- BillingService ‚Üí billing-svc (namespace: finance, port: 3003)
- PaymentService ‚Üí payment-svc (namespace: finance, port: 3015)
- AuthService ‚Üí auth-svc (namespace: customer, port: 3000)
- CustomerService ‚Üí customer-svc (namespace: customer, port: 3001)
- CRMOrchestratorService ‚Üí crm-orchestrator (namespace: customer, port: 3002)
- APIGatewayService ‚Üí api-gateway (namespace: platform, port: 3099)

**INTENT TYPES:**
- "business_model_change": Chuy·ªÉn ƒë·ªïi t·ª´ model n√†y sang model kh√°c
- "business_model_expansion": M·ªü r·ªông ƒë·ªÉ h·ªó tr·ª£ nhi·ªÅu models
- "update": C·∫≠p nh·∫≠t config c·ªßa services hi·ªán t·∫°i
- "scale": Thay ƒë·ªïi s·ªë l∆∞·ª£ng replicas

**OUTPUT FORMAT:**
Return ONLY valid JSON in this exact format:
{
  "proposal_text": "Detailed explanation in Vietnamese about what changes are needed",
  "changeset": {
    "model": "BusinessModel|MultiBusinessModel|SubscriptionPlan|etc",
    "features": [
      {"key": "business_model", "value": "retail|subscription|freemium|multi"},
      {"key": "other_config_key", "value": "config_value"}
    ],
    "impacted_services": ["ServiceName1", "ServiceName2", ...]
  },
  "metadata": {
    "intent": "business_model_change|business_model_expansion|update|scale",
    "confidence": 0.85-0.99,
    "risk": "low|medium|high",
    "from_model": "retail|subscription|etc (if applicable)",
    "to_model": "subscription|multi|etc (if applicable)"
  }
}

**EXAMPLES:**

Example 1 - Retail to Subscription:
Input: "Chuy·ªÉn s·∫£n ph·∫©m Premium Plan sang subscription 199k/th√°ng"
Output: {
  "changeset": {
    "model": "BusinessModel",
    "features": [
      {"key": "business_model", "value": "subscription"},
      {"key": "subscription_price", "value": 199000}
    ],
    "impacted_services": ["SubscriptionService", "PromotionService", "BillingService", "PaymentService", "CatalogueService"]
  },
  "metadata": {
    "intent": "business_model_change",
    "from_model": "retail",
    "to_model": "subscription"
  }
}

Example 2 - Multi-Model with multiple products:
Input: "H·ªó tr·ª£ 2 retail products, 1 subscription, v√† 1 freemium"
Output: {
  "changeset": {
    "model": "MultiBusinessModel",
    "features": [
      {"key": "business_model", "value": "multi"},
      {"key": "supported_models", "value": "retail,subscription,freemium"},
      {"key": "retail_products_count", "value": 2},
      {"key": "subscription_plans_count", "value": 1},
      {"key": "freemium_enabled", "value": true}
    ],
    "impacted_services": ["OrderService", "InventoryService", "SubscriptionService", "PromotionService", "CatalogueService", "BillingService", "PaymentService", "APIGatewayService", "AuthService"]
  },
  "metadata": {
    "intent": "business_model_expansion",
    "to_model": "multi",
    "note": "SHARED SERVICE PATTERN: Each service in impacted_services list will be deployed ONCE (e.g., 1 OrderService handles both retail products, 1 SubscriptionService handles subscription + freemium)"
  }
}

Example 3 - Freemium with Add-ons:
Input: "T·∫°o g√≥i Freemium mi·ªÖn ph√≠ v·ªõi 3 add-on t√≠nh ph√≠: Extra Storage (50k/th√°ng), AI Assistant (100k/th√°ng), Priority Support (30k/th√°ng)"
Output: {
  "changeset": {
    "model": "FreemiumWithAddons",
    "features": [
      {"key": "business_model", "value": "freemium"},
      {"key": "base_plan_price", "value": 0},
      {"key": "addons_enabled", "value": true},
      {"key": "addon_extra_storage_price", "value": 50000},
      {"key": "addon_ai_assistant_price", "value": 100000},
      {"key": "addon_priority_support_price", "value": 30000}
    ],
    "impacted_services": ["SubscriptionService", "BillingService", "PaymentService", "CatalogueService", "AuthService"]
  },
  "metadata": {
    "intent": "business_model_change",
    "to_model": "freemium_addon",
    "billing_mode": "addon_only"
  }
}

Return ONLY the JSON, no markdown code blocks, no additional text.`;

@Injectable()
export class LlmOrchestratorService {
  [x: string]: any;
  private provider = (process.env.LLM_PROVIDER || 'deepseek').toLowerCase();
  private geminiClient: GoogleGenerativeAI;
  private useRAG = process.env.USE_RAG === 'true'; // üëà Feature flag

  constructor(
    private codeSearchService: CodeSearchService,
    private validator: LlmOutputValidator,
    private k8sIntegrationService: K8sIntegrationService,
  ) {
    this.geminiClient = new GoogleGenerativeAI(
      process.env.GEMINI_API_KEY || '',
    );
  }

  async ask(
    message: string,
    tenant: string = 't-unknown',
    role: string = 'guest',
    lang: 'vi' | 'en' = 'vi',
  ): Promise<LlmChatResponse> {
     // üëá RAG: T√¨m code li√™n quan
    let codeContext = '';
    if (this.useRAG) {
      const relevantCode = await this.codeSearchService.searchRelevantCode(message, 3);
      
      if (relevantCode.length > 0) {
        codeContext = '\n\n=== RELEVANT CODE CONTEXT ===\n';
        relevantCode.forEach((code, idx) => {
          codeContext += `\n[${idx + 1}] ${code.file_path} (${code.chunk_type}${code.name ? `: ${code.name}` : ''})\n`;
          codeContext += `Score: ${code.score.toFixed(3)}\n`;
          codeContext += '```\n' + code.content.substring(0, 1000) + '\n```\n';
        });
        codeContext += '=== END CONTEXT ===\n';
      }
    }

    const content =
      this.provider === 'ollama'
        ? await this.callOllama(message, tenant, role, lang)
        : this.provider === 'deepseek'
          ? await this.callDeepSeek(message, tenant, role, lang)
          : await this.callGemini(message, tenant, role, lang);

    // Clean code fence if present
    let cleaned = content.trim();
    if (cleaned.startsWith('```')) {
      cleaned = cleaned
        .replace(/^```[a-zA-Z]*\n?/, '')
        .replace(/```$/, '')
        .trim();
    }
    if (cleaned.endsWith('```')) {
      cleaned = cleaned
        .replace(/^```[a-zA-Z]*\n?/, '')
        .replace(/```$/, '')
        .trim();
    }

    // Write to file for debugging
    try {
      const dir = path.resolve(process.cwd(), 'llm_output');
      await mkdir(dir, { recursive: true });
      const outputPath = path.join(dir, `${Date.now()}_raw.json`);
      await writeFile(outputPath, cleaned, 'utf8');
      console.log(`[LLM] Wrote clean JSON to: ${outputPath}`);
    } catch (err) {
      console.error('[LLM] Failed to write output file:', err);
    }

    // Parse and validate JSON
    let parsed: unknown;
    try {
      parsed = JSON.parse(cleaned);
    } catch {
      const match = String(cleaned).match(/\{[\s\S]*\}/);
      if (!match) throw new Error('LLM did not return valid JSON');
      parsed = JSON.parse(match[0]);
    }

    const validated = LLMReplySchema.parse(parsed);

    // üëà ADD: Business logic validation
    const validationResult = this.validator.validate(validated);
    
    if (!validationResult.isValid) {
      throw new Error(
        `LLM output validation failed:\n${validationResult.errors.join('\n')}`,
      );
    }
    
    // Log warnings if any
    if (validationResult.warnings.length > 0) {
      console.warn('[LLM Validator] Warnings:', validationResult.warnings.join('; '));
    }
    
    // Log metadata
    if (validationResult.metadata) {
      console.log('[LLM Validator] Metadata:', JSON.stringify(validationResult.metadata, null, 2));
    }

    // Convert value to string for gRPC (proto expects string)
    const response: LlmChatResponse = {
      proposal_text: validated.proposal_text,
      changeset: {
        model: validated.changeset.model,
        features: validated.changeset.features.map((f) => ({
          key: f.key,
          value: String(f.value), // Convert to string for proto
        })),
        impacted_services: validated.changeset.impacted_services,
      },
      metadata: validated.metadata,
    };

    // üöÄ AUTO-TRIGGER K8S DEPLOYMENT VIA KAFKA
    // Automatically publish deployment event after successful LLM processing
    try {
      const autoDeployEnabled = process.env.AUTO_DEPLOY_ENABLED === 'true';
      const dryRunDefault = process.env.DEFAULT_DRY_RUN !== 'false'; // Default: true
      
      if (autoDeployEnabled || dryRunDefault) {
        // Trigger deployment in background (don't wait)
        this.k8sIntegrationService.triggerDeployment(response, dryRunDefault)
          .catch(err => {
            console.error('[LLM] Failed to trigger K8s deployment:', err.message);
          });
      }
    } catch (error) {
      // Don't fail the LLM request if deployment trigger fails
      console.error('[LLM] Error triggering deployment:', error instanceof Error ? error.message : String(error));
    }

    return response;
  }

  // -------------------------------
  // DeepSeek (OpenAI-compatible)
  // -------------------------------
  private async callDeepSeek(
    message: string,
    tenant: string,
    role: string,
    lang: string,
  ): Promise<string> {
    const base = process.env.DEEPSEEK_BASE_URL || 'https://api.deepseek.com/v1';
    const model = process.env.DEEPSEEK_MODEL || 'deepseek-chat';
    const key = process.env.DEEPSEEK_API_KEY;
    if (!key) throw new Error('Missing DEEPSEEK_API_KEY');

    const res = await fetch(`${base}/chat/completions`, {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${key}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model,
        temperature: 0.2,
        messages: [
          { role: 'system', content: SYSTEM_PROMPT },
          {
            role: 'user',
            content: `tenant_id=${tenant}; role=${role}; lang=${lang};\n\nY√™u c·∫ßu: ${message}`,
          },
        ],
      }),
    });

    if (!res.ok) throw new Error(`DeepSeek ${res.status}: ${await res.text()}`);
    const data: any = await res.json();
    return data?.choices?.[0]?.message?.content ?? '{}';
  }

  // -------------------------------
  // Ollama (Local)
  // -------------------------------
  private async callOllama(
    message: string,
    tenant: string,
    role: string,
    lang: string,
  ): Promise<string> {
    const base = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    const model = process.env.OLLAMA_MODEL || 'llama3.1';

    const res = await fetch(`${base}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        stream: false,
        messages: [
          { role: 'system', content: SYSTEM_PROMPT },
          {
            role: 'user',
            content: `tenant_id=${tenant}; role=${role}; lang=${lang};\n\nY√™u c·∫ßu: ${message}`,
          },
        ],
      }),
    });

    if (!res.ok) throw new Error(`Ollama ${res.status}: ${await res.text()}`);
    const data: any = await res.json();
    return data?.message?.content ?? '{}';
  }

  // -------------------------------
  // Gemini (Google API)
  // -------------------------------
  private async callGemini(
    message: string,
    tenant: string,
    role: string,
    lang: string,
    codeContext: string = '',
  ): Promise<string> {
    const modelName = process.env.GEMINI_MODEL || 'gemini-2.5-flash'; // (Gemini 2.5 Flash ch∆∞a c√≥, c√≥ th·ªÉ √Ω b·∫°n l√† 1.5)
    
    // 1. C·∫•u h√¨nh model v·ªõi System Prompt
    const model = this.geminiClient.getGenerativeModel({
      model: modelName,
      systemInstruction: SYSTEM_PROMPT, // <-- Ch·ªâ d·∫´n h·ªá th·ªëng ƒë·∫∑t ·ªü ƒë√¢y
    });

    // 2. B·∫Øt ƒë·∫ßu chat (kh√¥ng c·∫ßn history v√¨ ƒë√¢y l√† 1 shot)
    const chat = model.startChat();

    // 3. T·∫°o n·ªôi dung c·ªßa User
     const userPrompt = `tenant_id=${tenant}; role=${role}; lang=${lang};
${codeContext}

Y√™u c·∫ßu: ${message}`;

    // 4. G·ª≠i tin nh·∫Øn c·ªßa User
    const result = await chat.sendMessage(userPrompt);

    return result.response.text() || '{}';
  }
}
